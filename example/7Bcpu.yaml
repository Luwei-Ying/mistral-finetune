# data
data:
  instruct_data: "/Users/luweiying/Documents/mistral-finetune/content/data/sampled_project_skill_question.jsonl"
  data: ""  # Optionally fill with pretraining data 
  eval_instruct_data: ""  # Optionally fill

# model
model_id_or_path: "/Users/luweiying/Documents/mistral-7B-v0.3"  # Change to downloaded path
lora:
  rank: 64

# optim
seq_len: 32768
batch_size: 1
max_steps: 100
optim:
  lr: 6.e-5
  weight_decay: 0.1
  pct_start: 0.05

# other
seed: 0
log_freq: 1
eval_freq: 100
no_eval: True
ckpt_freq: 100

save_adapters: True  # Save LoRA adapters only, to avoid saving the entire model.

run_dir: ""  # Fill

wandb:
  project: None  # Disable wandb
  run_name: None
  key: None
  offline: True